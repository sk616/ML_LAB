Python 3.6.3 (v3.6.3:2c5fed8, Oct  3 2017, 17:26:49) [MSC v.1900 32 bit (Intel)] on win32
Type "copyright", "credits" or "license()" for more information.
>>> 
========================== RESTART: E:\newann\bp.py ==========================

 The input Data Set :
 [[2.7810836, 2.550537003, 0], [1.465489372, 2.362125076, 0], [3.396561688, 4.400293529, 0], [1.38807019, 1.850220317, 0], [3.06407232, 3.005305973, 0], [7.627531214, 2.759262235, 1], [5.332441248, 2.088626775, 1], [6.922596716, 1.77106367, 1], [8.675418651, -0.242068655, 1], [7.673756466, 3.508563011, 1]]

 Number of Inputs :
 2

 Number of Outputs :
 2

 The initialised Neural Network:


 Layer[1] Node[1]:
 {'weights': [0.4560342718892494, 0.4478274870593494, -0.4434486322731913]}

 Layer[1] Node[2]:
 {'weights': [-0.41512800484107837, 0.33549887812944956, 0.2359699890685233]}

 Layer[2] Node[1]:
 {'weights': [0.1697304014402209, -0.1918635424108558, 0.10594416567846243]}

 Layer[2] Node[2]:
 {'weights': [0.10680173364083789, 0.08120401711200309, -0.3416171297451944]}

 Network Training Begins:

>epoch=0, lrate=0.500, error=5.278
>epoch=1, lrate=0.500, error=5.122
>epoch=2, lrate=0.500, error=5.006
>epoch=3, lrate=0.500, error=4.875
>epoch=4, lrate=0.500, error=4.700
>epoch=5, lrate=0.500, error=4.466
>epoch=6, lrate=0.500, error=4.176
>epoch=7, lrate=0.500, error=3.838
>epoch=8, lrate=0.500, error=3.469
>epoch=9, lrate=0.500, error=3.089
>epoch=10, lrate=0.500, error=2.716
>epoch=11, lrate=0.500, error=2.367
>epoch=12, lrate=0.500, error=2.054
>epoch=13, lrate=0.500, error=1.780
>epoch=14, lrate=0.500, error=1.546
>epoch=15, lrate=0.500, error=1.349
>epoch=16, lrate=0.500, error=1.184
>epoch=17, lrate=0.500, error=1.045
>epoch=18, lrate=0.500, error=0.929
>epoch=19, lrate=0.500, error=0.831
>epoch=20, lrate=0.500, error=0.748
>epoch=21, lrate=0.500, error=0.678
>epoch=22, lrate=0.500, error=0.618
>epoch=23, lrate=0.500, error=0.566
>epoch=24, lrate=0.500, error=0.521
>epoch=25, lrate=0.500, error=0.482
>epoch=26, lrate=0.500, error=0.448
>epoch=27, lrate=0.500, error=0.417
>epoch=28, lrate=0.500, error=0.391
>epoch=29, lrate=0.500, error=0.367
>epoch=30, lrate=0.500, error=0.345
>epoch=31, lrate=0.500, error=0.326
>epoch=32, lrate=0.500, error=0.308
>epoch=33, lrate=0.500, error=0.292
>epoch=34, lrate=0.500, error=0.278
>epoch=35, lrate=0.500, error=0.265
>epoch=36, lrate=0.500, error=0.253
>epoch=37, lrate=0.500, error=0.242
>epoch=38, lrate=0.500, error=0.231
>epoch=39, lrate=0.500, error=0.222
>epoch=40, lrate=0.500, error=0.213
>epoch=41, lrate=0.500, error=0.205
>epoch=42, lrate=0.500, error=0.197
>epoch=43, lrate=0.500, error=0.190
>epoch=44, lrate=0.500, error=0.184
>epoch=45, lrate=0.500, error=0.177
>epoch=46, lrate=0.500, error=0.172
>epoch=47, lrate=0.500, error=0.166
>epoch=48, lrate=0.500, error=0.161
>epoch=49, lrate=0.500, error=0.156
>epoch=50, lrate=0.500, error=0.151
>epoch=51, lrate=0.500, error=0.147
>epoch=52, lrate=0.500, error=0.143
>epoch=53, lrate=0.500, error=0.139
>epoch=54, lrate=0.500, error=0.135
>epoch=55, lrate=0.500, error=0.132
>epoch=56, lrate=0.500, error=0.128
>epoch=57, lrate=0.500, error=0.125
>epoch=58, lrate=0.500, error=0.122
>epoch=59, lrate=0.500, error=0.119
>epoch=60, lrate=0.500, error=0.116
>epoch=61, lrate=0.500, error=0.114
>epoch=62, lrate=0.500, error=0.111
>epoch=63, lrate=0.500, error=0.109
>epoch=64, lrate=0.500, error=0.106
>epoch=65, lrate=0.500, error=0.104
>epoch=66, lrate=0.500, error=0.102
>epoch=67, lrate=0.500, error=0.100
>epoch=68, lrate=0.500, error=0.098
>epoch=69, lrate=0.500, error=0.096
>epoch=70, lrate=0.500, error=0.094
>epoch=71, lrate=0.500, error=0.092
>epoch=72, lrate=0.500, error=0.091
>epoch=73, lrate=0.500, error=0.089
>epoch=74, lrate=0.500, error=0.087
>epoch=75, lrate=0.500, error=0.086
>epoch=76, lrate=0.500, error=0.084
>epoch=77, lrate=0.500, error=0.083
>epoch=78, lrate=0.500, error=0.081
>epoch=79, lrate=0.500, error=0.080
>epoch=80, lrate=0.500, error=0.079
>epoch=81, lrate=0.500, error=0.077
>epoch=82, lrate=0.500, error=0.076
>epoch=83, lrate=0.500, error=0.075
>epoch=84, lrate=0.500, error=0.074
>epoch=85, lrate=0.500, error=0.073
>epoch=86, lrate=0.500, error=0.072
>epoch=87, lrate=0.500, error=0.071
>epoch=88, lrate=0.500, error=0.070
>epoch=89, lrate=0.500, error=0.069
>epoch=90, lrate=0.500, error=0.068
>epoch=91, lrate=0.500, error=0.067
>epoch=92, lrate=0.500, error=0.066
>epoch=93, lrate=0.500, error=0.065
>epoch=94, lrate=0.500, error=0.064
>epoch=95, lrate=0.500, error=0.063
>epoch=96, lrate=0.500, error=0.062
>epoch=97, lrate=0.500, error=0.061
>epoch=98, lrate=0.500, error=0.061
>epoch=99, lrate=0.500, error=0.060

 Network Training Ends:


 Final Neural Network :

 Layer[1] Node[1]:
 {'weights': [1.3614748137524701, -1.762447105294539, -1.1998944376872873], 'output': 0.954211705258084, 'delta': 0.0007539950089143327}

 Layer[1] Node[2]:
 {'weights': [-1.673900818606045, 2.392380217208838, 0.9765917286838552], 'output': 0.030747013416738658, 'delta': -0.0006639089898122631}

 Layer[2] Node[1]:
 {'weights': [-2.6347423758908324, 3.5805767226285563, -0.3797147382140517], 'output': 0.05837839160892833, 'delta': -0.0032090809111934102}

 Layer[2] Node[2]:
 {'weights': [2.8122446402096815, -3.442656521569256, 0.21911197197313165], 'output': 0.9423367536373211, 'delta': 0.0031333168048555964}
>>> 
